# Copyright 2017-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.

from __future__ import print_function
import argparse, json, os, itertools, random, shutil
import time
import re
from tqdm import tqdm
from operator import itemgetter
import datetime
from pathlib import Path
import question_engine as qeng

"""
Generate synthetic questions and answers for CLEVR images. Input is a single
JSON file containing ground-truth scene information for all images, and output
is a single JSON file containing all generated questions, answers, and programs.

Questions are generated by expanding templates. Each template contains a single
program template and one or more text templates, both with the same set of typed
slots; by convention <Z> = Size, <C> = Color, <M> = Material, <S> = Shape.

Program templates may contain special nodes that expand into multiple functions
during instantiation; for example a "filter" node in a program template will
expand into a combination of "filter_size", "filter_color", "filter_material",
and "filter_shape" nodes after instantiation, and a "filter_unique" node in a
template will expand into some combination of filtering nodes followed by a
"unique" node.

Templates are instantiated using depth-first search; we are looking for template
instantiations where (1) each "unique" node actually refers to a single object,
(2) constraints in the template are satisfied, and (3) the answer to the question
passes our rejection sampling heuristics.

To efficiently handle (1) and (2), we keep track of partial evaluations of the
program during each step of template expansion. This together with the use of
composite nodes in program templates (filter_unique, relate_filter_unique) allow
us to efficiently prune the search space and terminate early when we know that
(1) or (2) will be violated.
"""




parser = argparse.ArgumentParser()

# Inputs
# parser.add_argument('--input_scene_file', default='../output/cyclist_scenes.json',
#     help="JSON file containing ground-truth scene information for all images " +
#          "from render_images.py")

parser.add_argument('--input_scene_file', default='../output/scenes/unicycle/test',
    help="Filepath containing ground-truth scene information for all images " +
         "from render_images.py")


parser.add_argument('--metadata_file', default='metadata.json',
    help="JSON file containing metadata about functions")
parser.add_argument('--synonyms_json', default='synonyms.json',
    help="JSON file defining synonyms for parameter values")
parser.add_argument('--cyclic_json', default='cyclic.json',
    help="JSON file defining cyclic names for parameter values")
parser.add_argument('--template_dir', default='CycliST_1.0_templates',
    help="Directory containing JSON templates for questions")

# Output
parser.add_argument('--output_questions_file',
    default='../output/cycliST_questions.json',
    help="The output file to write containing generated questions")

# Control which and how many images to process
parser.add_argument('--scene_start_idx', default=0, type=int,
    help="The image at which to start generating questions; this allows " +
         "question generation to be split across many workers")
parser.add_argument('--num_scenes', default=0, type=int,
    help="The number of images for which to generate questions. Setting to 0 " +
         "generates questions for all scenes in the input file starting from " +
         "--scene_start_idx")
parser.add_argument('--scene_fps', default=32, type=int,
    help="FPS")

# Control the number of questions per image; we will attempt to generate
# templates_per_image * instances_per_template questions per image.
parser.add_argument('--templates_per_image', default=10, type=int,
    help="The number of different templates that should be instantiated " +
         "on each image")
parser.add_argument('--instances_per_template', default=1, type=int,
    help="The number of times each template should be instantiated on an image")
# control for the templates to be used
parser.add_argument('-t', '--templates', nargs='+', default=[])



# Misc
parser.add_argument('--reset_counts_every', default=250, type=int,
    help="How often to reset template and answer counts. Higher values will " +
         "result in flatter distributions over templates and answers, but " +
         "will result in longer runtimes.")
parser.add_argument('--verbose', action='store_true',
    help="Print more verbose output")
parser.add_argument('--time_dfs', action='store_true',
    help="Time each depth-first search; must be given with --verbose")
parser.add_argument('--profile', action='store_true',
    help="If given then run inside cProfile")



special_nodes = {
    'filter_unique', 'filter_count', 'filter_exist', 'filter',
    'relate_filter_globally', 'relate_filter_unique_globally', 'relate_filter_count_globally',
    'relate_filter_exist_globally',
    'relate_filter_finally', 'relate_filter_unique_finally', 'relate_filter_count_finally',
    'relate_filter_exist_finally'
}
cyclic_nodes = {
    'filter_orbit', 'filter_linear', 'filter_motion',
    'filter_enlarge', 'filter_rotate', 'filter_change_color',
    'filter_enlarge_period', 'filter_orbit_period', 'filter_color_period'
}



def precompute_filter_options(scene_struct, metadata):
    """
    
    
    
    """

    attribute_map = {}

    # define attributes keys
    if metadata['dataset'] == 'CycliST-v1.0':
        attr_keys = ['size', 'color', 'material','change_color','enlarge', 'rotate','mesh', 'orbit', 'linear']
    else:
        assert False, 'Unrecognized dataset'

    # Precompute masks (binary masks for all object attribute combinations)
    masks = []
    for i in range(2 ** len(attr_keys)):
        mask = []
        for j in range(len(attr_keys)):
            mask.append((i // (2 ** j)) % 2)
        masks.append(mask)


    for object_idx, obj in enumerate(scene_struct['objects']):
        if metadata['dataset'] == 'CycliST-v1.0':
            # set all attribute keys to the attribute value or False if not present
            keys = [obj[k] if k in obj else False for k in attr_keys] 
            #set all attribute keys corresponding to cycles to True 
            if 'scenes' in obj:
                if 'cycles' in obj:
                    #color change
                    if 'recolor' in obj['cycles']['scenes']:
                        keys[3] = True
                    # enlarging
                    if 'resize' in obj['cycles']['scenes']:
                        keys[4] = True
                    # rotating
                    if 'rotate' in obj['cycles']['scenes']:
                        keys[5] = True
                    # orbit
                    if 'orbit' in obj['cycles']['scenes']:
                        keys[7] = True
                    # linear
                    if 'linear' in obj['cycles']['scenes']:
                        keys[8] = True
        # TODO would it help to set them to None instead of False?
          
        # overlay the binary masks on the attribute keys list
        for mask in masks:
            masked_key = []
            for a, b in zip(keys, mask):
                if b == 1:
                    masked_key.append(a)
                else:
                    masked_key.append(None)
            masked_key = tuple(masked_key)
            if masked_key not in attribute_map:
                attribute_map[masked_key] = set()
            attribute_map[masked_key].add(object_idx)

    #attribute_map now has keys of the form ('small', None, 'Metal', None, None, None, 'Cube', None, None) 
    # mapping to a set of objects {1,3}

    
    # color with change_color tag
    # attribute_map = {k: v for k, v in attribute_map.items() if (k[1] == None) or (k[1] != None and k[3] != None)}
    # # size with enlarge tag
    # attribute_map = {k: v for k, v in attribute_map.items() if ((k[0] != 'small') or (k[0] == 'small' and k[4] != None))}
    #degenerate

    combinations = ([2],[3,4,5], [4,5], [3,5], [3,4], [3], [4], [5])
    # 3 = change color
    # 4 = enlarging
    # 5 = rotating
    
    #reduce the attribute map size for redundancy
    filtered = attribute_map.copy()
    for attributes_key, obj in attribute_map.items():
        # if attributes_key[3] == attributes_key[4] == attributes_key[5] == None:
        #     continue
        # else:
        for combi in combinations:
            cur_k = list(attributes_key)
            # if 3 in combi:
            #   if cur_k[1] != None:
            #     continue
            # if 4 in combi:
            #   if cur_k[0] != None:
            #     continue
            
            #set property chaning cycles attrbutes to None
            for index in combi:
                cur_k[index] = None
            if tuple(attributes_key) == tuple(cur_k):
                continue
            cur_v = attribute_map[tuple(cur_k)]
            if cur_v == obj:
                filtered.pop(tuple(attributes_key), None)
                    
    #add the filtered options to the scene struct
    scene_struct['_filter_options'] = filtered


def find_filter_options(object_idxs, scene_struct, metadata):
    # Keys are tuples (size, color, shape, material) (where some may be None)
    # and values are lists of object idxs that match the filter criterion

    if '_filter_options' not in scene_struct:
        precompute_filter_options(scene_struct, metadata)

    attribute_map = {}
    object_idxs = set(object_idxs)
    for k, vs in scene_struct['_filter_options'].items():
        attribute_map[k] = sorted(list(object_idxs & vs))
    return attribute_map


def add_empty_filter_options(attribute_map, metadata, num_to_add):
    # Add some filtering criterion that do NOT correspond to objects

    if metadata['dataset'] == 'CycliST-v1.0':
        attr_keys = ['Size', 'Color', 'Material', 'change_color', 'enlarge',  'rotate', 'Shape']
    else:
        assert False, 'Unrecognized dataset'

    attr_vals = [metadata['types'][t] + [None] for t in attr_keys]
    if '_filter_options' in metadata:
        attr_vals = metadata['_filter_options']

    target_size = len(attribute_map) + num_to_add
    while len(attribute_map) < target_size:
        k = (random.choice(v) for v in attr_vals)
        if k not in attribute_map:
            attribute_map[k] = []


#TODO is this used somewhere?
# def find_relate_filter_options(object_idx, scene_struct, metadata,
#     unique=False, include_zero=False, trivial_frac=0.1):
#     options = {}
#     if '_filter_options' not in scene_struct:
#         precompute_filter_options(scene_struct, metadata)

#     # TODO: Right now this is only looking for nontrivial combinations; in some
#     # cases I may want to add trivial combinations, either where the intersection
#     # is empty or where the intersection is equal to the filtering output.
#     trivial_options = {}
#     for relationship in scene_struct['relationships']:
#         related = set(scene_struct['relationships'][relationship][object_idx])
#         for filters, filtered in scene_struct['_filter_options'].items():
#             intersection = related & filtered
#             trivial = (intersection == filtered)
#             if unique and len(intersection) != 1: continue
#             if not include_zero and len(intersection) == 0: continue
#             if trivial:
#                 trivial_options[(relationship, filters)] = sorted(list(intersection))
#             else:
#                 options[(relationship, filters)] = sorted(list(intersection))

#     N, f = len(options), trivial_frac
#     num_trivial = int(round(N * f / (1 - f)))
#     trivial_options = list(trivial_options.items())
#     random.shuffle(trivial_options)
#     for k, v in trivial_options[:num_trivial]:
#         options[k] = v

#     return options

def find_relate_filter_options_temporal(object_idx, scene_struct, metadata,
    unique=False, include_zero=False, trivial_frac=0.1, globally = False):
    options = {}
    precompute_filter_options(scene_struct, metadata)

    # TODO: Right now this is only looking for nontrivial combinations; in some
    # cases I may want to add trivial combinations, either where the intersection
    # is empty or where the intersection is equal to the filtering output.
    trivial_options = {}

    # for frame, relationships in enumerate(scene_struct['relationships_seq']):
        # for relationship in relationships:
    for relation in scene_struct['relationships']:
        for frame_key in scene_struct['relationships'][relation]:
            related = set()

        # related = set(relationships[relationship][object_idx])
        related = set()
        # Collect related objects for the current frame
        for entry in scene_struct['relationships'][relation][frame_key]:
            if entry[0] == object_idx:
                related.add(entry[1])

        # Compare related objects with precomputed filter options
        for filters, filtered in scene_struct['_filter_options'].items():
            intersection = related & filtered
            trivial = (intersection == filtered)
            key = (relation, filters)
            # key = (relationship, filters)

            intersection_list = sorted(list(intersection))

            if unique and len(intersection) != 1:
                continue
            if not include_zero and len(intersection) == 0:
                continue

            if globally:
                # Combine results across all frames
                if trivial:
                    if key not in trivial_options:
                        trivial_options[key] = intersection
                    else:
                        trivial_options[key] &= intersection
                else:
                    if key not in options:
                        options[key] = intersection
                    else:
                        options[key] &= intersection
            else:
                # Store results for each frame independently
                if trivial:
                    if key not in trivial_options:
                        trivial_options[key] = []
                    if not trivial_options[key] or intersection_list not in trivial_options[key]:
                        trivial_options[key].append(intersection_list)
                else:
                    if key not in options:
                        options[key] = []
                    if not options[key] or intersection_list not in options[key]:
                        options[key].append(intersection_list)

        # If globally, convert sets to lists
        if globally:
            for k, v in trivial_options.items():
                trivial_options[k] = [sorted(list(v))]
            for k, v in options.items():
                options[k] = [sorted(list(v))]

        # Add trivial options based on the trivial_frac parameter
        N, f = len(options), trivial_frac
        num_trivial = int(round(N * f / (1 - f)))
        trivial_options = list(trivial_options.items())
        random.shuffle(trivial_options)
        for k, v in trivial_options[:num_trivial]:
            options[k] = v

        return options

def node_shallow_copy(node):
    new_node = {
      'type': node['type'],
      'inputs': node['inputs'],
    }
    if 'side_inputs' in node:
        new_node['side_inputs'] = node['side_inputs']
    return new_node


def other_heuristic(text, param_vals):
    """
    Post-processing heuristic to handle the word "other"
    """
    if ' other ' not in text and ' another ' not in text:
        return text
    target_keys = {
      '<Z>',  '<C>',  '<M>',  '<S>',
      '<Z2>', '<C2>', '<M2>', '<S2>',
    }
    if param_vals.keys() != target_keys:
        return text
    key_pairs = [
      ('<Z>', '<Z2>'),
      ('<C>', '<C2>'),
      ('<M>', '<M2>'),
      ('<S>', '<S2>'),
    ]
    remove_other = False
    for k1, k2 in key_pairs:
        v1 = param_vals.get(k1, None)
        v2 = param_vals.get(k2, None)
        if v1 != '' and v2 != '' and v1 != v2:
            print('other has got to go! %s = %s but %s = %s'
                  % (k1, v1, k2, v2))
            remove_other = True
            break
    if remove_other:
        if ' other ' in text:
            text = text.replace(' other ', ' ')
        if ' another ' in text:
            text = text.replace(' another ', ' a ')
    return text


def mask_filter_options(side_inputs, filter_option_keys, param_name_to_type):
    """
    if only a subeset of "<Z> <C> <M> <CC> <RO> <E> <S>" is given produce a mask with the active parameters
    """

    attr_keys = ['Size', 'Color', 'Material', 'change_color', 'enlarge',  'rotate', 'Shape', 'orbit', 'linear']
    indices = []
    for side_input in side_inputs:
        attr_name = param_name_to_type[side_input]
        #get index of the attribute in attr_keys
        if attr_name in attr_keys:
            attr_index = attr_keys.index(attr_name)
            indices.append(attr_index)

    selected_filter_option_keys = [itemgetter(*indices)(filter_option_key) for filter_option_key in filter_option_keys]

    #return unique elements
    return list(set(selected_filter_option_keys))


def enrich_scene_struct(scene_struct):
    """
    In the scene_struct json the cycles are defined in property. 
    To easily access wherever we have a cycle we add an additional property for each cycle
    to be able to quickly check if the cycle is present for the object.
    
    """

    # enricht scene struct with the number of frames
    scene_struct['num_frame'] = int(scene_struct['fps']*scene_struct['duration'])

    #enrich scene struct with cyclic properties
    for scene in scene_struct['objects']:

        scene['orbit'] = None
        scene['linear'] = None
        scene['change_color'] = None
        scene['enlarge'] = None
        scene['rotate'] = None
        
        if not "cycles" in scene:
            continue

        # add orbit = True if orbit is in cycles
        if 'orbit' in scene['cycles']:
            scene['orbit'] = scene['center']

        if 'linear' in scene['cycles']:
            scene['linear'] = True
        if 'recolor' in scene['cycles']:
            scene['change_color'] = True
        if 'resize' in scene['cycles']:
            scene['enlarge'] = True
        if 'rotate' in scene['cycles']:
            scene['rotate'] = True

    return scene_struct


def check_constraints(state, template, verbose, outputs, param_name_to_type):
    """
    Checks if all constraints are satisfied for the current state. 
    NEQ (Not Equal):Ensures that two parameters (p1 and p2) have different values. If both values are defined and equal, the state is skipped.
    NULL: Ensures that a parameter (p) is "null" (empty or default).
    OUT_NEQ (Output Not Equal):
    Ensures that the outputs of two nodes (i and j) are not equal. If the outputs are the same, the state is skipped.
    
    Constraints are given in the template in the form:
    {
        "params": ["<C>"],
        "type": "NULL" },
    {
        "params": [1, 3], #input params correspond to state outputs of node 1 and 3
        "type": "OUT_NEQ"}
    """
    
     
    skip_state = False
    for constraint in template['constraints']:
        if constraint['type'] == 'NEQ':
            p1, p2 = constraint['params']
            v1, v2 = state['vals'].get(p1), state['vals'].get(p2)
            if v1 is not None and v2 is not None and v1 != v2:
                if verbose:
                    print('skipping due to NEQ constraint')
                    print(constraint)
                    print(state['vals'])
                skip_state = True
                
        elif constraint['type'] == 'NULL':
            #For Shape type, check if the value is 'thing'.
            #For other types, check if the value is an empty string. If the condition is not met, the state is skipped.
            p = constraint['params'][0]
            p_type = param_name_to_type[p]
            v = state['vals'].get(p)
            if v is not None:
                skip = False
                if p_type == 'Shape' and v != 'thing': skip = True
                if p_type != 'Shape' and v != '': skip = True
                if skip:
                    if verbose:
                        print('skipping due to NULL constraint')
                        print(constraint)
                        print(state['vals'])
                    skip_state = True
                    
        elif constraint['type'] == 'OUT_NEQ':
            i, j = constraint['params']
            i = state['input_map'].get(i, None)
            j = state['input_map'].get(j, None)
            if i is not None and j is not None and outputs[i] == outputs[j]:
                if verbose:
                    print('skipping due to OUT_NEQ constraint')
                    print(outputs[i])
                    print(outputs[j])
                skip_state = True
        else:
            assert False, 'Unrecognized constraint type "%s"' % constraint['type']
    return skip_state


# def check_answer_distribution():

def instantiate_templates_dfs(scene_struct, template, metadata, answer_counts,
                              synonyms, cyclic, max_instances=None, verbose=False):
    """
    This function instantiates a template usin depth first search using the scene information and a template.

    """
    
    # maps placeholders like "<Z>" to their attributes "Size"
    param_name_to_type = {p['name']: p['type'] for p in template['params']}

    # enrich the scene struct to quickly check the presence of cycles.
    scene_struct = enrich_scene_struct(scene_struct)

    initial_state = {
      'nodes': [node_shallow_copy(template['nodes'][0])],
      'vals': {},
      'input_map': {0: 0},
      'next_template_node': 1,
    }
    states = [initial_state]
    final_states = []
    
    #iterate over all states in the DFS
    while states:
        state = states.pop()

        # Check to make sure the current state is valid
        q = {'nodes': state['nodes']}
        outputs = qeng.answer_question(q, metadata, scene_struct, all_outputs=True)
        # if the answer of this node sequence is Invalid skip it
        answer = outputs[-1]
        if answer == '__INVALID__': continue
        
        #print output nodes
        if args.verbose:
            print("-----------")
            for node in state['nodes']:
                if 'side_inputs' in node:
                    print(node['type'], "IN:",node['inputs'], "+", node['side_inputs']," -> ", node['_output'])
                else:
                    print(node['type'], "IN:",node['inputs']," -> ", node['_output'])
        
    
        #check if all constraints are fulfiled
        skip_state = check_constraints(state, template, verbose, outputs, param_name_to_type)
        # we have a constraint that is not fulfilled. 
        if skip_state:
            continue

        # We have already checked to make sure the answer is valid, so if we have
        # processed all the nodes in the template then the current state is a valid
        # question, so add it if it passes our rejection sampling tests (we want an euqal distribution of question types).
        if state['next_template_node'] == len(template['nodes']):
            # Use our rejection sampling heuristics to decide whether we should
            # keep this template instantiation
            cur_answer_count = answer_counts[answer]
            answer_counts_sorted = sorted(answer_counts.values())
            median_count = answer_counts_sorted[len(answer_counts_sorted) // 2]
            median_count = max(median_count, 5)
            if cur_answer_count > 1.1 * answer_counts_sorted[-2]:
                if verbose: print('skipping due to second count')
                continue
            if cur_answer_count > 5.0 * median_count:
                if verbose: print('skipping due to median')
                continue

            # If the template contains a raw relate node then we need to check for
            # degeneracy at the end #TODO What does degeneracy mean in this case?
            has_relate = any(n['type'] == 'relate' for n in template['nodes'])
            if has_relate:
                degen = qeng.is_degenerate(q, metadata, scene_struct, answer=answer,
                                           verbose=verbose)
                if degen:
                    continue

            answer_counts[answer] += 1
            state['answer'] = answer
            final_states.append(state)
            if max_instances is not None and len(final_states) == max_instances:
                break
            continue

        # Otherwise fetch the next node from the template
        # Make a shallow copy so cached _outputs don't leak ... this is very nasty
        next_node = template['nodes'][state['next_template_node']]
        next_node = node_shallow_copy(next_node)


        # process all special nodes
        if next_node['type'] in special_nodes:
            # all relate filter nodes
            if next_node['type'].startswith('relate_filter'):
                unique = next_node['type'].startswith('relate_filter_unique')
                include_zero = next_node['type'].startswith('relate_filter_count') or next_node['type'].startswith('relate_filter_exist')
                globally = next_node['type'].endswith('globally')
                filter_options = find_relate_filter_options_temporal(answer, scene_struct, metadata,
                                    unique=unique, include_zero=include_zero, globally=globally)
            # all non relate filters
            else:
                filter_options = find_filter_options(answer, scene_struct, metadata)
                if next_node['type'] == 'filter':
                    # Remove null filter
                    # size, color ,material, shape, enlarge, change_color, rotate
                    filter_options.pop((None, None, None, None, None, None), None)
                
                # Get rid of all filter options that don't result in a single object
                if next_node['type'] == 'filter_unique':
                    filter_options = {k: v for k, v in filter_options.items()
                                      if len(v) == 1}
                else:
                    # Add some filter options that do NOT correspond to the scene
                    if next_node['type'] == 'filter_exist':
                        # For filter_exist we want an equal number that do and don't
                        num_to_add = len(filter_options)
                    elif next_node['type'] == 'filter_count' or next_node['type'] == 'filter':
                        # For filter_count add nulls equal to the number of singletons
                        num_to_add = sum(1 for k, v in filter_options.items() if len(v) == 1)
                    add_empty_filter_options(filter_options, metadata, num_to_add)

            filter_option_keys = list(filter_options.keys())

            #remove unused filters from list of all <S> <C> <M> <CC> <RO> <E>
            filter_option_keys = mask_filter_options(next_node['side_inputs'], filter_option_keys, param_name_to_type)

            # Shuffle the filter options so that we don't always expand them in the same order
            random.shuffle(filter_option_keys)
            for k in filter_option_keys:
                new_nodes = []
                cur_next_vals = {k: v for k, v in state['vals'].items()}
                next_input = state['input_map'][next_node['inputs'][0]]
                filter_side_inputs = next_node['side_inputs']

                if next_node['type'].startswith('relate'):
                    param_name = next_node['side_inputs'][0] # First one should be relate
                    filter_side_inputs = next_node['side_inputs'][1:]
                    param_type = param_name_to_type[param_name]
                    assert param_type == 'Relation'
                    param_val = k[0]
                    k = k[1]
                    new_nodes.append({
                      'type': 'relate_finally' if next_node['type'].endswith('finally') else 'relate_globally',
                      'inputs': [next_input],
                      'side_inputs': [param_val],
                    })
                    cur_next_vals[param_name] = param_val
                    next_input = len(state['nodes']) + len(new_nodes) - 1

                # add filter nodes for each side node parameter
                for param_name, param_val in zip(filter_side_inputs, k):
                    param_type = param_name_to_type[param_name]
                    filter_type = 'filter_%s' % param_type.lower()
                    if param_val is not None:
                        new_nodes.append({
                          'type': filter_type,
                          'inputs': [next_input],
                          'side_inputs': [param_val],
                        })
                        # if args.verbose:
                        #     print('Adding filter %s with value %s' % (filter_type, param_val))
                        cur_next_vals[param_name] = param_val
                        next_input = len(state['nodes']) + len(new_nodes) - 1
                    elif param_val is None:
                        if metadata['dataset'] == 'CycliST-v1.0' and param_type == 'Shape':
                            param_val = 'thing'
                        else:
                            param_val = ''
                        cur_next_vals[param_name] = param_val
                input_map = {k: v for k, v in state['input_map'].items()}
                extra_type = None
                if 'unique' in next_node['type']:
                    extra_type = 'unique'
                if 'count' in next_node['type']:
                    extra_type = 'count'
                if 'exist' in next_node['type']:
                    extra_type = 'exist'
                if extra_type is not None:
                    new_nodes.append({
                      'type': extra_type,
                      'inputs': [input_map[next_node['inputs'][0]] + len(new_nodes)],
                    })
                input_map[state['next_template_node']] = len(state['nodes']) + len(new_nodes) - 1
                states.append({
                  'nodes': state['nodes'] + new_nodes,
                  'vals': cur_next_vals,
                  'input_map': input_map,
                  'next_template_node': state['next_template_node'] + 1,
                })
        
        # process nodes that already have a side input
        elif 'side_inputs' in next_node:
            # If the next node has template parameters, expand them out
            # TODO: Generalize this to work for nodes with more than one side input
            assert len(next_node['side_inputs']) == 1, 'NOT IMPLEMENTED'

            # Use metadata to figure out domain of valid values for this parameter.
            # Iterate over the values in a random order; then it is safe to bail
            # from the DFS as soon as we find the desired number of valid template
            # instantiations.
            param_name = next_node['side_inputs'][0]
            param_type = param_name_to_type[param_name]
            param_vals = metadata['types'][param_type][:]
            random.shuffle(param_vals)
            for val in param_vals:
                input_map = {k: v for k, v in state['input_map'].items()}
                input_map[state['next_template_node']] = len(state['nodes'])
                cur_next_node = {
                  'type': next_node['type'],
                  'inputs': [input_map[idx] for idx in next_node['inputs']],
                  'side_inputs': [val],
                }
                cur_next_vals = {k: v for k, v in state['vals'].items()}
                cur_next_vals[param_name] = val

                states.append({
                  'nodes': state['nodes'] + [cur_next_node],
                  'vals': cur_next_vals,
                  'input_map': input_map,
                  'next_template_node': state['next_template_node'] + 1,
                })
                
        
        # process cyclic special nodes
        elif next_node['type'] in cyclic_nodes:
            param_type = next_node['type'].partition('_')[-1]
            param_vals = set()
            
            #if node filter_motion
            if param_type == 'motion':
                param_vals.add(True)
            # filter_<cycle>
            # get all present attributes vals for the selected attribute. 
            else:
                for obj in scene_struct['objects'][:]:
                    param_vals.add(obj[param_type])
            
            # remove all False entries         
            param_vals = list(filter(lambda a: a != False, param_vals))


            if next_node['type'].endswith('enlarge_period'):
                raise NotImplementedError
            random.shuffle(param_vals)
            
            # create a filter_<cycle> node with every cycle attribute
            for val in param_vals:
                input_map = {k: v for k, v in state['input_map'].items()}
                input_map[state['next_template_node']] = len(state['nodes'])
                cur_next_node = {
                  'type': next_node['type'],
                  'inputs': [input_map[idx] for idx in next_node['inputs']],
                  'side_inputs': [val],
                }

                states.append({
                  'nodes': state['nodes'] + [cur_next_node],
                  'vals': state['vals'],
                  'input_map': input_map,
                  'next_template_node': state['next_template_node'] + 1,
                })
        #if the node type is not a special or cyclic node, nor has a side input
        else:
            input_map = {k: v for k, v in state['input_map'].items()}
            input_map[state['next_template_node']] = len(state['nodes'])
            next_node = {
              'type': next_node['type'],
              'inputs': [input_map[idx] for idx in next_node['inputs']],
            }
            states.append({
              'nodes': state['nodes'] + [next_node],
              'vals': state['vals'],
              'input_map': input_map,
              'next_template_node': state['next_template_node'] + 1,
            })

    # Actually instantiate the template with the solutions we've found
    text_questions, structured_questions, answers = [], [], []
    for state in final_states:
        structured_questions.append(state['nodes'])
        answers.append(state['answer'])
        text = random.choice(template['text'])
        for name, val in state['vals'].items():
            if param_name_to_type[name] in ["change_color", "enlarge", "rotate"]:
                if val != '':
                    neg = 'not ' if val == 'False' else ''
                    val = neg + random.choice(cyclic[param_name_to_type[name]])
            if val in synonyms:
                val = random.choice(synonyms[val])
            text = text.replace(name, val)
            text = ' '.join(text.split())
        text = replace_optionals(text)
        text = ' '.join(text.split())
        text = other_heuristic(text, state['vals'])
        text_questions.append(text)

    return text_questions, structured_questions, answers



def replace_optionals(s):
    """
    Each substring of s that is surrounded in square brackets is treated as
    optional and is removed with probability 0.5. For example the string

    "A [aa] B [bb]"

    could become any of

    "A aa B bb"
    "A  B bb"
    "A aa B "
    "A  B "

    with probability 1/4.
    """
    pat = re.compile(r'\[([^\[]*)\]')

    while True:
        match = re.search(pat, s)
        if not match:
            break
        i0 = match.start()
        i1 = match.end()
        if random.random() > 0.5:
            s = s[:i0] + match.groups()[0] + s[i1:]
        else:
            s = s[:i0] + s[i1:]
    return s


def reset_counts(args, metadata, templates):
    # Maps a template (filename, index) to the number of questions we have
    # so far using that template
    template_counts = {}
    
    # Maps a template (filename, index) to a dict mapping the answer to the
    # number of questions so far of that template type with that answer
    template_answer_counts = {}
    node_type_to_dtype = {n['name']: n['output'] for n in metadata['functions']}
    for key, template in templates.items():
        template_counts[key[:2]] = 0
        final_node_type = template['nodes'][-1]['type']
        final_dtype = node_type_to_dtype[final_node_type]
        answers = metadata['types'][final_dtype]
        if final_dtype == 'Bool':
            answers = [True, False]
        if final_dtype == 'Prime':
            answers = list(range(1, int(args.scene_fps) * 8 + 1))
        if final_dtype == 'Integer':
            if metadata['dataset'] == 'CycliST-v1.0':
                answers = list(range(0, 11))
        template_answer_counts[key[:2]] = {}
        for a in answers:
            template_answer_counts[key[:2]][a] = 0
    return template_counts, template_answer_counts


def group_scene_files(dir_path):
    
    all_scenes = []
    #load all paths in the directory and open all json files from it
    for filename in os.listdir(dir_path):
        #check if the file is a json
        if filename.endswith('.json'):
            try:
                with open(Path(dir_path, filename)) as json_file:
                    data = json.load(json_file)
                    all_scenes.append(data)
            except:
                print("Error opening file:", filename)
                continue
    
    scene_info = {
            "date": datetime.datetime.now().strftime("%dd-%m-%y"),
            "version": "1.0",
            "split": dir_path.split("/")[-1],
            "license": "Creative Commons Attribution (CC-BY 4.0)"
    }
    return all_scenes, scene_info


def main(args):
    """
    Main loop to generate questions from templates and scenes.
    This function reads the metadata file, loads templates from disk, and
    generates questions by instantiating templates on the input scenes.
    It uses depth-first search to explore all possible instantiations of the
    templates, applying various heuristics to filter out invalid or redundant
    questions. The generated questions are then stored in a list, which can be
    saved to a file or processed further.
    The function also handles various command-line arguments to control the
    behavior of the question generation process, such as the number of scenes
    to process, whether to reset counts periodically, and whether to print
    verbose output.
    """
    
    
    # load the metadata file. 
    # The file contains attributes types and their domain, as well as function definitios inlcluding their input output and side inputs.
    with open(args.metadata_file, 'r') as f:
        metadata = json.load(f)
        dataset = metadata['dataset']
        if dataset != 'CycliST-v1.0':
            raise ValueError('Unrecognized dataset "%s"' % dataset)

    # Read synonyms file
    with open(args.synonyms_json, 'r') as f:
        synonyms = json.load(f)
    with open(args.cyclic_json, 'r') as f:
        cyclic = json.load(f)

    #dict of functions that maps from name to input, outputs, side_inputs
    functions_by_name = {} 
    for f in metadata['functions']:
        functions_by_name[f['name']] = f
    metadata['_functions_by_name'] = functions_by_name

    # Load question templates from disk
    # Key (filename, file_idx):  text, nodes, params, contraints
    num_loaded_templates = 0
    templates = {}
    for template_filename in os.listdir(args.template_dir):
        if not template_filename.endswith('.json'): continue
        with open(os.path.join(args.template_dir, template_filename), 'r') as f:
            for i, template in enumerate(json.load(f)):
                # Exclude templates containing relate nodes
                # if any(node['type'].startswith('relate') for node in template['nodes']):
                #   continue
                key = (template_filename, i)
                templates[key] = template
    
    
    #filter out templates that align with one of the template filters
    if len(args.templates) > 0:
        templates= {(k1, k2):v for (k1,k2), v in templates.items() if any(substring in k1 for substring in args.templates)}
        print('Filtered templates to only include %d templates' % len(templates))
    
    print('Read %d templates from disk' % len(templates))


    #init the template and template answer counts 
    template_counts, template_answer_counts = reset_counts(args, metadata, templates)
    total_template_counts = template_counts
    total_template_answer_counts = template_answer_counts

    # Read file containing input scenes
    # select with scene_stadt_id and total number of scenes 
    all_scenes = []
    if os.path.isfile(args.input_scene_file):
        with open(args.input_scene_file, 'r') as f:
            print("Loading scenes from scene file")
            scene_data = json.load(f)
            all_scenes = scene_data['scenes']
            scene_info = scene_data['info']
    else: # we need to group it
        print("Loading scenes files from directory")
        all_scenes, scene_info = group_scene_files(args.input_scene_file)        
        
        
    begin = args.scene_start_idx
    if args.num_scenes > 0:
        end = args.scene_start_idx + args.num_scenes
        all_scenes = all_scenes[begin:end]
    else:
        all_scenes = all_scenes[begin:]


    questions = []
    scene_count = 0
    print('Generating questions for %d scenes' % len(all_scenes))
    for i, scene in enumerate(tqdm(all_scenes)):
        scene_struct = scene
        print('Processing scene %d / %d' % (i + 1, len(all_scenes)))
        video_filename = scene['video_file']
        print('starting video %s (%d / %d)'
              % (video_filename, i + 1, len(all_scenes)))

        #reset the scene count every t steps to allow for sampling all answers
        if scene_count % args.reset_counts_every == 0 and scene_count != 0:
            print('resetting counts')
                
            #save current counts         
            for k in template_counts.keys():
                total_template_counts[k] += template_counts[k]
                for k2 in template_answer_counts[k].keys():
                    total_template_answer_counts[k][k2] += template_answer_counts[k][k2]
            
            template_counts, template_answer_counts = reset_counts(args, metadata, templates)
        scene_count += 1

        # Order templates by the number of questions we have so far for those
        # templates. This is a simple heuristic to give a flat distribution over templates.
        templates_items = list(templates.items())
        templates_items = sorted(templates_items,
                            key=lambda x: template_counts[x[0][:2]])
        num_instantiated = 0
        for (template_filename, idx), template in templates_items:
            if args.verbose:
                print('trying template ', template_filename, idx)
            if args.time_dfs and args.verbose:
                tic = time.time()

            # create questions and answers using the template
            ts, qs, ans = instantiate_templates_dfs(
                            scene_struct,
                            template,
                            metadata,
                            template_answer_counts[(template_filename, idx)],
                            synonyms,
                            cyclic,
                            max_instances=args.instances_per_template,
                            verbose=args.verbose)


            if args.time_dfs and args.verbose:
                toc = time.time()
                print('that took ', toc - tic)

            # a template can have multiple instantiations. 
            # Add each of them and their corresponding answer to the list of questions.
            video_index = int(os.path.splitext(video_filename)[0].split('_')[-1])
            for t, q, a in zip(ts, qs, ans):
                questions.append({
                  'split': scene_info['split'],
                  'video_filename': video_filename,
                  'video_index': video_index,
                  'video': os.path.splitext(video_filename)[0],
                  'question': t,
                  'program': q,
                  'answer': a,
                  'template_filename': template_filename,
                  'question_family_index': idx,
                  'question_index': len(questions),
                })
            
            # keep track of num of instantiated templates and template counts
            if len(ts) > 0:
                if args.verbose:
                    print('got one!')
                num_instantiated += 1
                template_counts[(template_filename, idx)] += 1
            elif args.verbose:
                print('did not get any =(')
            
            # stop instantiating new templates for the selected scene if we have enough 
            if num_instantiated >= args.templates_per_image:
                break

    # Change "side_inputs" to "value_inputs" in all functions of all functional
    # programs. My original name for these was "side_inputs" but I decided to
    # change the name to "value_inputs" for the public CLEVR release. I should
    # probably go through all question generation code and templates and rename,
    # but that could be tricky and take a while, so instead I'll just do it here.
    # To further complicate things, originally functions without value inputs did
    # not have a "side_inputs" field at all, and I'm pretty sure this fact is used
    # in some of the code above; however in the public CLEVR release all functions
    # have a "value_inputs" field, and it's an empty list for functions that take
    # no value inputs. Again this should probably be refactored, but the quick and
    # dirty solution is to keep the code above as-is, but here make "value_inputs"
    # an empty list for those functions that do not have "side_inputs". Gross.
    for q in questions:
        for f in q['program']:
            if 'side_inputs' in f:
                f['value_inputs'] = f['side_inputs']
                del f['side_inputs']
            else:
                f['value_inputs'] = []
    #save current counts         
    for k in template_counts.keys():
        total_template_counts[k] += template_counts[k]
        for k2 in template_answer_counts[k].keys():
            total_template_answer_counts[k][k2] += template_answer_counts[k][k2]
    d1= {}
    for k in  total_template_counts.keys():
        d1["".join([k[0],"_",str(k[1])])] = total_template_counts[k]
    
    d2= {}
    for k in  total_template_answer_counts.keys():
        d2["".join([k[0],"_",str(k[1])])] = total_template_answer_counts[k]
    


    # make sure that the directory of the filepath exists. Create it otherwise
    os.makedirs(os.path.dirname(args.output_questions_file), exist_ok=True)

    # store all questions to the questions file
    with open(args.output_questions_file, 'w') as f:
        print('Writing output to %s' % args.output_questions_file)
        json.dump({
            'info': scene_info,
            'questions': questions,
            'template_counts': d1,
            'template_answer_counts':d2
            }, f)


if __name__ == '__main__':
    args = parser.parse_args()
    if args.profile:
        import cProfile
        cProfile.run('main(args)')
    else:
        main(args)
