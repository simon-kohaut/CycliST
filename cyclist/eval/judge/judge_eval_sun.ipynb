{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Environment variables loaded:\n",
      "HF_HOME /pfss/mlde/workspaces/mlde_wsp_PI_Kersting/LLaVA-cake/Cyclist/hfcache\n",
      "HUGGINGFACE_HUB_CACHE /pfss/mlde/workspaces/mlde_wsp_PI_Kersting/LLaVA-cake/Cyclist/hfcache\n",
      "WANDB_PROJECT llava-changes\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.load_env_vars import load_env\n",
    "load_env()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataloader import CyListSceneUnderstandingDataset, CyListVQADataset\n",
    "import json\n",
    "device ='cuda:2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datalaoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 42, 1080, 1920, 3])\n",
      "<class 'str'>\n",
      "[\n",
      "    {\n",
      "        \"shape\": \"cube\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"purple\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cone\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"cyan\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"True\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": 0,\n",
      "            \"change_colors\": \"blue\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"blue\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"True\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "data_path=\"/pfss/mlde/workspaces/mlde_wsp_PI_Kersting/LLaVA-cake/Cyclist/output\"\n",
    "cyc_dataset = CyListSceneUnderstandingDataset(data_path)\n",
    "\n",
    "torch_dataset = DataLoader(cyc_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "#print one example\n",
    "for i, data in enumerate(torch_dataset):\n",
    "    print(data[0].shape)\n",
    "    print(type(data[1][0]))\n",
    "    print(data[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP not installed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c75655e47f4a14950dab42c6478a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from data.model_loader import load_model_and_processor\n",
    "\n",
    "model_id = \"llava-hf/llava-onevision-qwen2-72b-ov-chat-hf\"\n",
    "model_id = \"llava-hf/llava-onevision-qwen2-7b-ov-chat-hf\"\n",
    "\n",
    "model, processor = load_model_and_processor(model_id, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval loop\n",
    "load different models and aggregate them in model loader file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scene json\n",
      "[\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"large\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"red\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"gray\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"rubber\",\n",
      "        \"color\": \"gray\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": 0,\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cone\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"rubber\",\n",
      "        \"color\": \"red\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"True\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"rubber\",\n",
      "        \"color\": \"yellow\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"True\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"large\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"blue\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "[\n",
      "    {\n",
      "        \"shape\": \"cube\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"purple\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cone\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"cyan\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"True\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": 0,\n",
      "            \"change_colors\": \"blue\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"blue\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"True\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "[\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"large\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"red\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"gray\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"rubber\",\n",
      "        \"color\": \"gray\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": 0,\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cone\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"rubber\",\n",
      "        \"color\": \"red\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"True\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"small\",\n",
      "        \"material\": \"rubber\",\n",
      "        \"color\": \"yellow\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"True\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"shape\": \"cylinder\",\n",
      "        \"size\": \"large\",\n",
      "        \"material\": \"metal\",\n",
      "        \"color\": \"blue\",\n",
      "        \"transformation\": {\n",
      "            \"enlarges\": \"False\",\n",
      "            \"rotates\": \"False\",\n",
      "            \"orbits\": \"False\",\n",
      "            \"change_colors\": \"False\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Hey, can you describe the scene in the video?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for i, data in enumerate(torch_dataset):\n",
    "    videos = data[0] #[BS, T, H, W, C] -> [[2, 42, 1080, 1920, 3]) \n",
    "    scene_json = data[1]\n",
    "    \n",
    "    \n",
    "    # processor accepts tensors with channel [T, H, W,C] -> (20, 240, 320, 3)\n",
    "\n",
    "    \n",
    "    #produce prompts\n",
    "    prompts = []\n",
    "    for _ in scene_json:\n",
    "        # Each \"content\" is a list of dicts and you can add image/video/text modalities\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": query},\n",
    "                    {\"type\": \"video\"},\n",
    "                    ],\n",
    "            },\n",
    "        ]\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "\n",
    "    inputs = processor(text=prompts, videos=torch.unbind(videos, dim=0), padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "\n",
    "    # Generate the output tokens\n",
    "    generate_kwargs = {\"max_new_tokens\": 500, \"do_sample\": False, \"top_p\": 0.9}\n",
    "    output = model.generate(**inputs, **generate_kwargs)\n",
    "\n",
    "    # Find the eos token id in the inputs\n",
    "    eos_token_id = processor.tokenizer.eos_token_id\n",
    "    eos_token_pos = torch.where(inputs[\"input_ids\"] == eos_token_id)[1]\n",
    "\n",
    "    # Extract only the newly generated tokens\n",
    "    new_tokens = []\n",
    "    for i, eos_pos in enumerate(eos_token_pos):\n",
    "        new_tokens.append(output[i, eos_pos + 1:])\n",
    "\n",
    "    # Decode the newly generated tokens\n",
    "    generated_text = processor.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "    print(\"True scene json\")\n",
    "    for scene in scene_json:\n",
    "        print(scene) \n",
    "    print(\"Generated scene json\")\n",
    "    for text in generated_text:\n",
    "        print(text)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
