{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Environment variables loaded:\n",
      "HF_HOME /pfss/mlde/workspaces/mlde_wsp_PI_Kersting/LLaVA-cake/Cyclist/hfcache\n",
      "HUGGINGFACE_HUB_CACHE /pfss/mlde/workspaces/mlde_wsp_PI_Kersting/LLaVA-cake/Cyclist/hfcache\n",
      "WANDB_PROJECT llava-changes\n",
      "TOKENIZERS_PARALLELISM true\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.load_env_vars import load_env\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt= \"\"\"\n",
    "Hey, can you judge this prediction for my visual question answering dataset? The Answers can be given as a whole sentence or simple yes/no or attribute like 'color' or 'shape' or 'size' etc.\n",
    "\n",
    "If the answer matches the Groundtruth give a score of one, else give a score of zero. Treat yes/no as a binary classification proble equal to 1 for yes and 0 for no, or true and false respectively.\n",
    "\n",
    "For example, if the prediction is 'red' and the groundtruth is 'red', you should give a score of 1. If the prediction is 'red' and the groundtruth is 'blue', you should give a score of 0.\n",
    "\n",
    "return a json object with the following format:\n",
    "{{\n",
    "    \"prediction\": \"red\",\n",
    "    \"groundtruth\": \"red\",\n",
    "    \"score\": 1\n",
    "}}\n",
    "\n",
    "Prediction:\n",
    "{}\n",
    "\n",
    "GT:\n",
    "{}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = \"\"\"\n",
    "assistant\n",
    "No.\n",
    "\"\"\"\n",
    "gt = \"False\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from judge_utils import query_sglang_llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_sglang_llama3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[0;32m~/Cyclist/src/eval/llm_judge/judge_utils.py:42\u001b[0m, in \u001b[0;36mquery_sglang_llama3\u001b[0;34m(pred, gt, max_tokens, judge_prompt)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_sglang_llama3\u001b[39m(pred, gt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m, judge_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     37\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     38\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     39\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     40\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful AI assistant helping me score the predictions of a model.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     41\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 42\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mjudge_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m(pred, gt)},\n\u001b[1;32m     43\u001b[0m         ],\n\u001b[1;32m     44\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     45\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m,\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "response = query_sglang_llama3(pred, gt)\n",
    "\n",
    "print(response)\n",
    "\n",
    "import json\n",
    "import re\n",
    "json_response = re.search(r'{(.|\\r?\\n)*}', response).group()\n",
    "json_response = json.loads(json_response)\n",
    "print(json_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt= \"\"\"\n",
    "Hey, can you judge this prediction for my visual question answering dataset? The Answers can be given as a whole sentence or simple yes/no or attribute like 'color' or 'shape' or 'size' etc.\n",
    "\n",
    "If the answer matches the Groundtruth give a score of one, else give a score of zero. Treat yes/no as a binary classification proble equal to 1 for yes and 0 for no, or true and false respectively.\n",
    "\n",
    "For example, if the prediction is 'red' and the groundtruth is 'red', you should give a score of 1. If the prediction is 'red' and the groundtruth is 'blue', you should give a score of 0. An empty answer should get a score of 0.\n",
    "\n",
    "return a json object with the following format:\n",
    "{{\n",
    "    \"prediction\": \"red\",\n",
    "    \"groundtruth\": \"red\",\n",
    "    \"score\": 1\n",
    "}}\n",
    "\n",
    "Prediction:\n",
    "{}\n",
    "\n",
    "GT:\n",
    "{}\n",
    "\n",
    "\"\"\"\n",
    "import openai\n",
    "client = openai.Client(\n",
    "    base_url=\"http://127.0.0.1:30000/v1\", api_key=\"EMPTY\")\n",
    "\n",
    "pred = \"\"\"\n",
    "assistant\n",
    "No.\n",
    "\"\"\"\n",
    "gt = \"False\"\n",
    "\n",
    "preds = [pred, pred, pred]\n",
    "gts = [gt, gt, gt]\n",
    "def query_sglang_llama3_batched(preds, gts):\n",
    "    responses = client.chat.completions.create(\n",
    "        model=\"default\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": judge_prompt.format(pred, gt)}\n",
    "            for pred, gt in zip(preds, gts)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return [choice.message.content for choice in responses.choices]\n",
    "\n",
    "\n",
    "\n",
    "responses = query_sglang_llama3_batched(preds, gts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the judgment:\n",
      "\n",
      "{\n",
      "    \"prediction\": \"No\",\n",
      "    \"groundtruth\": \"False\",\n",
      "    \"score\": 1\n",
      "}\n",
      "\n",
      "Explanation: The prediction \"No\" matches the groundtruth \"False\", which is equivalent to 0 in a binary classification problem. So, the score is 1.\n"
     ]
    }
   ],
   "source": [
    "for i in responses:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
